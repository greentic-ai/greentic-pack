// @generated by packc -- DO NOT EDIT BY HAND.
#![allow(dead_code)]
#![allow(clippy::all)]

pub static MANIFEST_CBOR: &[u8] = &[
    0xa6, 0x67, 0x70, 0x61, 0x63, 0x6b, 0x5f, 0x69, 0x64, 0x70, 0x67, 0x72,
    0x65, 0x65, 0x6e, 0x74, 0x69, 0x63, 0x2e, 0x71, 0x61, 0x2e, 0x64, 0x65,
    0x6d, 0x6f, 0x67, 0x76, 0x65, 0x72, 0x73, 0x69, 0x6f, 0x6e, 0x65, 0x30,
    0x2e, 0x31, 0x2e, 0x30, 0x6a, 0x63, 0x72, 0x65, 0x61, 0x74, 0x65, 0x64,
    0x5f, 0x61, 0x74, 0x78, 0x1e, 0x32, 0x30, 0x32, 0x35, 0x2d, 0x31, 0x31,
    0x2d, 0x31, 0x30, 0x54, 0x32, 0x32, 0x3a, 0x30, 0x36, 0x3a, 0x33, 0x33,
    0x2e, 0x31, 0x35, 0x34, 0x36, 0x32, 0x32, 0x36, 0x35, 0x38, 0x5a, 0x65,
    0x66, 0x6c, 0x6f, 0x77, 0x73, 0x82, 0xa6, 0x62, 0x69, 0x64, 0x6e, 0x71,
    0x61, 0x5f, 0x61, 0x6e, 0x73, 0x77, 0x65, 0x72, 0x5f, 0x66, 0x6c, 0x6f,
    0x77, 0x64, 0x74, 0x79, 0x70, 0x65, 0x69, 0x6d, 0x65, 0x73, 0x73, 0x61,
    0x67, 0x69, 0x6e, 0x67, 0x65, 0x73, 0x74, 0x61, 0x72, 0x74, 0x68, 0x63,
    0x61, 0x6c, 0x6c, 0x5f, 0x6c, 0x6c, 0x6d, 0x66, 0x73, 0x6f, 0x75, 0x72,
    0x63, 0x65, 0x78, 0x19, 0x66, 0x6c, 0x6f, 0x77, 0x73, 0x2f, 0x71, 0x61,
    0x5f, 0x61, 0x6e, 0x73, 0x77, 0x65, 0x72, 0x5f, 0x66, 0x6c, 0x6f, 0x77,
    0x2e, 0x79, 0x67, 0x74, 0x63, 0x66, 0x73, 0x68, 0x61, 0x32, 0x35, 0x36,
    0x78, 0x40, 0x38, 0x39, 0x66, 0x65, 0x36, 0x31, 0x63, 0x39, 0x32, 0x65,
    0x64, 0x65, 0x62, 0x62, 0x61, 0x32, 0x32, 0x32, 0x37, 0x64, 0x33, 0x34,
    0x36, 0x32, 0x63, 0x66, 0x64, 0x30, 0x65, 0x62, 0x64, 0x37, 0x37, 0x66,
    0x39, 0x34, 0x32, 0x34, 0x39, 0x37, 0x30, 0x38, 0x35, 0x66, 0x34, 0x35,
    0x63, 0x61, 0x35, 0x30, 0x35, 0x32, 0x61, 0x66, 0x61, 0x61, 0x66, 0x65,
    0x36, 0x36, 0x31, 0x34, 0x39, 0x35, 0x64, 0x73, 0x69, 0x7a, 0x65, 0x19,
    0x04, 0x1a, 0xa6, 0x62, 0x69, 0x64, 0x6f, 0x71, 0x61, 0x5f, 0x6f, 0x72,
    0x63, 0x68, 0x65, 0x73, 0x74, 0x72, 0x61, 0x74, 0x6f, 0x72, 0x64, 0x74,
    0x79, 0x70, 0x65, 0x69, 0x6d, 0x65, 0x73, 0x73, 0x61, 0x67, 0x69, 0x6e,
    0x67, 0x65, 0x73, 0x74, 0x61, 0x72, 0x74, 0x70, 0x63, 0x6f, 0x6c, 0x6c,
    0x65, 0x63, 0x74, 0x5f, 0x71, 0x75, 0x65, 0x73, 0x74, 0x69, 0x6f, 0x6e,
    0x66, 0x73, 0x6f, 0x75, 0x72, 0x63, 0x65, 0x78, 0x1a, 0x66, 0x6c, 0x6f,
    0x77, 0x73, 0x2f, 0x71, 0x61, 0x5f, 0x6f, 0x72, 0x63, 0x68, 0x65, 0x73,
    0x74, 0x72, 0x61, 0x74, 0x6f, 0x72, 0x2e, 0x79, 0x67, 0x74, 0x63, 0x66,
    0x73, 0x68, 0x61, 0x32, 0x35, 0x36, 0x78, 0x40, 0x64, 0x30, 0x38, 0x65,
    0x33, 0x66, 0x37, 0x66, 0x62, 0x35, 0x38, 0x65, 0x39, 0x32, 0x39, 0x31,
    0x32, 0x36, 0x37, 0x31, 0x39, 0x63, 0x66, 0x39, 0x38, 0x35, 0x36, 0x30,
    0x36, 0x33, 0x33, 0x64, 0x30, 0x61, 0x36, 0x30, 0x35, 0x64, 0x65, 0x38,
    0x63, 0x35, 0x37, 0x64, 0x37, 0x32, 0x63, 0x61, 0x35, 0x65, 0x33, 0x31,
    0x39, 0x64, 0x35, 0x31, 0x34, 0x63, 0x32, 0x30, 0x62, 0x37, 0x38, 0x62,
    0x64, 0x73, 0x69, 0x7a, 0x65, 0x19, 0x05, 0x1e, 0x69, 0x74, 0x65, 0x6d,
    0x70, 0x6c, 0x61, 0x74, 0x65, 0x73, 0x80, 0x70, 0x69, 0x6d, 0x70, 0x6f,
    0x72, 0x74, 0x73, 0x5f, 0x72, 0x65, 0x71, 0x75, 0x69, 0x72, 0x65, 0x64,
    0x85, 0x6a, 0x71, 0x61, 0x2e, 0x70, 0x72, 0x6f, 0x63, 0x65, 0x73, 0x73,
    0x69, 0x66, 0x6c, 0x6f, 0x77, 0x2e, 0x63, 0x61, 0x6c, 0x6c, 0x6f, 0x6c,
    0x6c, 0x6d, 0x2e, 0x6f, 0x70, 0x65, 0x6e, 0x61, 0x69, 0x2e, 0x63, 0x68,
    0x61, 0x74, 0x6e, 0x6d, 0x65, 0x73, 0x73, 0x61, 0x67, 0x69, 0x6e, 0x67,
    0x2e, 0x65, 0x6d, 0x69, 0x74, 0x69, 0x73, 0x74, 0x61, 0x74, 0x65, 0x2e,
    0x67, 0x65, 0x74
];

pub static FLOWS: &[(&'static str, &'static str)] = &[
    ("qa_answer_flow", "id: qa_answer_flow\ntitle: Specialist Answer Flow\ndescription: Call an LLM and emit multiple replies (reasoning + conclusion).\ntype: messaging\nstart: call_llm\n\nnodes:\n  call_llm:\n    llm.openai.chat:\n      model: in.model\n      system_prompt: >\n        You are an expert research companion. Show your reasoning, cite facts,\n        and keep answers actionable. When profile hints are available, weave\n        them in naturally.\n      messages:\n        - role: system\n          content: |\n            Profile hints:\n            {{#if in.profile}}\n            {{in.profile}}\n            {{else}}\n            (none supplied)\n            {{/if}}\n        - role: user\n          content: \"{{in.question}}\"\n        - role: user\n          content: \"{{in.context}}\"\n    routing:\n      - to: package_messages\n\n  package_messages:\n    flow.return:\n      payload:\n        - type: message\n          channel: debug\n          text: \"{{call_llm.payload.reasoning}}\"\n        - type: message\n          text: \"{{call_llm.payload.answer}}\"\n    routing:\n      - out: true\n"),
    ("qa_orchestrator", "id: qa_orchestrator\ntitle: QA Orchestrator\ndescription: Collect a user question, call a specialist subflow, and stream the reply.\ntype: messaging\nstart: collect_question\n\nparameters:\n  answer_flow_id: qa_answer_flow\n  state_key: \"qa-demo::profile\"\n  default_model: gpt-4o-mini\n\nnodes:\n  collect_question:\n    qa.process:\n      welcome: \"Hey there! I can dive deep on any topic. Give me a prompt and I\'ll brainstorm with you.\"\n      questions:\n        - id: q_problem\n          prompt: \"\u{1f680} What should we explore?\"\n          answer_type: text\n          max_words: 80\n        - id: q_context\n          prompt: \"Anything else I should know?\"\n          optional: true\n          answer_type: text\n          max_words: 80\n    routing:\n      - to: load_profile\n\n  load_profile:\n    state.get:\n      key: parameters.state_key\n      fallback: {}\n    routing:\n      - to: call_specialist\n\n  call_specialist:\n    flow.call:\n      flow_id: parameters.answer_flow_id\n      input:\n        model: parameters.default_model\n        question: collect_question.payload.q_problem\n        context: collect_question.payload.q_context\n        profile: load_profile.payload.value\n    routing:\n      - to: respond_to_user\n\n  respond_to_user:\n    messaging.emit:\n      messages: call_specialist.payload\n    routing:\n      - out: true\n"),
];

pub static TEMPLATES: &[(&'static str, &'static [u8])] = &[
];

